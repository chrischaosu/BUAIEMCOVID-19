{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts from 2020-03-03 to 2020-03-09.xls\n",
      "Posts from 2020-04-18 to 2020-04-18.xls\n",
      "Posts from 2020-04-29 to 2020-04-30.xls\n",
      "Posts from 2020-04-10 to 2020-04-11.xls\n",
      "Posts from 2020-04-19 to 2020-04-21.xls\n",
      "Posts from 2020-04-15 to 2020-04-17.xls\n",
      "Posts from 2020-05-01 to 2020-05-02.xls\n",
      "Posts from 2020-03-13 to 2020-03-16.xls\n",
      "Posts from 2020-02-14 to 2020-03-02.xls\n",
      "Posts from 2020-03-28 to 2020-03-30.xls\n",
      "Posts from 2020-04-26 to 2020-04-28.xls\n",
      "Posts from 2020-04-12 to 2020-04-14.xls\n",
      "Posts from 2020-03-10 to 2020-03-12.xls\n",
      "Posts from 2020-03-31 to 2020-04-01.xls\n",
      "Posts from 2020-03-19 to 2020-03-20.xls\n",
      "Posts from 2020-03-21 to 2020-03-23.xls\n",
      "Posts from 2020-04-22 to 2020-04-23.xls\n",
      "Posts from 2019-12-01 to 2020-02-13.xls\n",
      "Posts from 2020-03-24 to 2020-03-25.xls\n",
      "Posts from 2020-04-02 to 2020-04-04.xls\n",
      "Posts from 2020-04-08 to 2020-04-09.xls\n",
      "Posts from 2020-04-24 to 2020-04-25.xls\n",
      "Posts from 2020-03-26 to 2020-03-27.xls\n",
      "Posts from 2020-03-17 to 2020-03-18.xls\n",
      "Posts from 2020-04-05 to 2020-04-07.xls\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CHUNK FOR ENGLISH NEWS / MULTIPLE SPREADSHEETS\n",
    "import collections\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "docs_location = 'Downloads/cv_us_20191201_to_20200320/'\n",
    "content_full = []\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords = stopwords + ['coronavirus', 'covid', 'corona']\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "universal_count = collections.Counter()\n",
    "dfs = []\n",
    "\n",
    "# ^^ Here we're just importing libraries and instantiating some global variables.\n",
    "dont_stem = {'Sanders'}\n",
    "special_words = [\n",
    "'donald trump',\n",
    "'white house',\n",
    "'social distancing',\n",
    "'stock market',\n",
    "'new york',\n",
    "'joe biden',\n",
    "'bernie sanders',\n",
    "'world health organization',\n",
    "'face mask',\n",
    "'mike pence',\n",
    "'vice president',\n",
    "'hong kong',\n",
    "'united states',\n",
    "'diamond princess',\n",
    "'new hampshire',\n",
    "'whats happening',\n",
    "'lunar year',\n",
    "'los angeles',\n",
    "'san francisco',\n",
    "'elissa slotkin']\n",
    "\n",
    "urls_mainstream = [\n",
    "    'nytimes.com',\n",
    "    'washingtonpost.com',\n",
    "    'usatoday.com',\n",
    "    'wsj.com',\n",
    "    'newsweek.com',\n",
    "    'nbcnews.com',\n",
    "    'cbsnews.com',\n",
    "    'abcnews.go.com',\n",
    "    'cnn.com',\n",
    "    'pbs.org',\n",
    "    'npr.org',\n",
    "    'latimes.com',\n",
    "    'chicagotribune.com'\n",
    "]\n",
    "\n",
    "urls_conservative = [\n",
    "    'foxnews.com',\n",
    "    'breitbart.com',\n",
    "    'newsmax.com',\n",
    "    'theblaze.com',\n",
    "    'dailycaller.com',\n",
    "    'drudgereport.com'\n",
    "]\n",
    "\n",
    "urls_liberal = [\n",
    "    'msnbc.com',\n",
    "    'motherjones.com',\n",
    "    'theatlantic.com',\n",
    "    'huffingtonpost.com',\n",
    "    'vox.com',\n",
    "    'slate.com',\n",
    "    'buzzfeednews.com',\n",
    "    'dailykos.com'\n",
    "]\n",
    "\n",
    "def get_category_of_news_outlet(url):\n",
    "    for url_m in urls_mainstream:\n",
    "        if url_m in url:\n",
    "            return 'mainstream'\n",
    "    for url_c in urls_conservative:\n",
    "        if url_c in url:\n",
    "            return 'conservative'\n",
    "    for url_l in urls_liberal:\n",
    "        if url_l in url:\n",
    "            return 'liberal'\n",
    "    return None\n",
    "\n",
    "def substitute_special_words(content):\n",
    "    content = content.translate(str.maketrans('', '', ''.join(punctuation_no_underscore) + string.digits))\n",
    "    content = content.lower()\n",
    "    for w in special_words:\n",
    "        if w in content:\n",
    "            content = re.sub(w,  '_'.join(w.split()), content)\n",
    "            \n",
    "    return content\n",
    "\n",
    "punctuation_no_underscore = set(string.punctuation)\n",
    "punctuation_no_underscore.add('’')\n",
    "punctuation_no_underscore.add('”')\n",
    "punctuation_no_underscore.remove('_')\n",
    "\n",
    "# here we go through a directory containing all the Excel spreadsheets\n",
    "for doc in os.listdir(docs_location):\n",
    "    print(doc)\n",
    "    df = pd.read_excel(os.path.join(docs_location + doc))\n",
    "    # pd.read_excel seems to want to grab the header line, so we make sure to ignore that; \n",
    "    # column 'Unnamed: 3' is the actual article content.\n",
    "    content_not_clean_yet = df['Unnamed: 3'][1:]\n",
    "    title_not_clean_yet = df['Unnamed: 17'][1:]\n",
    "    # we remove punctuation here\n",
    "    content_not_clean_yet = [substitute_special_words(c) + substitute_special_words(t) \\\n",
    "                             for c, t in zip(content_not_clean_yet, title_not_clean_yet)]\n",
    "    content_no_punctuation = [[word for word in c.split() if re.match('[a-zA-Z0-9]+', word)] for c in content_not_clean_yet]\n",
    "    # here we remove stopwords\n",
    "    content_no_stopwords = [\n",
    "        [c for c in content if c not in stopwords]\n",
    "        for content in content_no_punctuation\n",
    "        \n",
    "    ]\n",
    "    # I'm using gensim's native lemmatizer to lemmatize our content\n",
    "    content_lemmatized = [\n",
    "        [lemmatizer.lemmatize(c) for c in content \\\n",
    "         if c != 'sanders' and c not in special_words] \n",
    "    for content in content_no_stopwords]\n",
    "    # here we get rid of short words\n",
    "    content_full = [[c for c in content if len(c) > 2] for content in content_lemmatized]\n",
    "    # get dates (splitting up by week of year)\n",
    "    # the timedelta is necessary to align so that Sunday is the first day of the week.\n",
    "    dates_full = list(df['Unnamed: 1'][1:].apply(lambda b: pd.to_datetime(b + pd.Timedelta(days=1)).week))\n",
    "    # and get string dates so we can confirm we're getting correct weeks\n",
    "    str_times = list(df['Unnamed: 1'][1:].apply(lambda b: pd.to_datetime(b).strftime('%Y%m%d')))\n",
    "    df = df[1:]\n",
    "    df['dates_full'] = dates_full\n",
    "    df['content_full'] = content_full\n",
    "    df['dt_str'] = str_times\n",
    "    df['political_leaning'] = df['Unnamed: 2'].apply(get_category_of_news_outlet)\n",
    "    dfs.append(df)\n",
    "    \n",
    "# df_final contains all data.\n",
    "df_final = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-6a42a1cb7361>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdocs_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Downloads/chinese_news/'\u001b[0m \u001b[0;31m# PUT THE FOLDER WHERE YOU ARE KEEPING THE SPREADSHEET HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_location\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mcontent_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content_seg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# here we get rid of short words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     return io.parse(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, engine)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0mformatting_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatting_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0mon_demand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_demand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0mragged_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mragged_rows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             )\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/xlrd/xlsx.py\u001b[0m in \u001b[0;36mopen_workbook_2007_xml\u001b[0;34m(zf, component_names, logfile, verbosity, use_mmap, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msst_fname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomponent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0mzflo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msst_fname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mx12sst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzflo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SST'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mzflo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/xlrd/xlsx.py\u001b[0m in \u001b[0;36mprocess_stream_iterparse\u001b[0;34m(self, stream, heading)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0melemno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0msst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sharedstrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msi_tag\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0melemno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melemno\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36miterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m                 \u001b[0mpullparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m             \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpullparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_and_return_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpullparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mSyntaxError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1628\u001b[0m         \u001b[0;34m\"\"\"Feed encoded data to parser.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raiseerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/private/tmp/python-20190327-98828-4pywv0/Python-3.7.3/Modules/pyexpat.c\u001b[0m in \u001b[0;36mEndElement\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/xml/etree/ElementTree.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(tag, event, append, end)\u001b[0m\n\u001b[1;32m   1510\u001b[0m                 \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStartElementHandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mevent_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m                 def handler(tag, event=event_name, append=append,\n\u001b[0m\u001b[1;32m   1513\u001b[0m                             end=self._end):\n\u001b[1;32m   1514\u001b[0m                     \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# RUN THIS CHUNK FOR CHINESE NEWS / SINGLE SPREADSHEET\n",
    "import collections\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "docs_location = 'Downloads/chinese_news/' # PUT THE FOLDER WHERE YOU ARE KEEPING THE SPREADSHEET HERE\n",
    "doc = os.listdir(docs_location)[0]\n",
    "df = pd.read_excel(os.path.join(docs_location + doc))\n",
    "content_series = df['content_seg'].apply(lambda b: [i.strip() for i in b.split()])\n",
    "# here we get rid of short words\n",
    "content_full = [[c for c in content if len(c) >= 2] for content in content_series]\n",
    "# get dates (splitting up by week of year)\n",
    "# dates_full = list(df['Unnamed: 1'][1:].apply(lambda b: pd.to_datetime(b).week))\n",
    "# inserting a dummy value because we want to look at all days, even though they're not the same week.\n",
    "dates_full = np.zeros(len(content_full))\n",
    "# and get string dates so we can confirm we're getting correct weeks\n",
    "str_times = list(df['Date (GMT)'].apply(lambda b: pd.to_datetime(b).strftime('%Y%m%d')))\n",
    "df['dates_full'] = dates_full\n",
    "df['content_full'] = content_full\n",
    "df['dt_str'] = str_times\n",
    "df_final = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_proportions(corpus, model, best_n_clusters):\n",
    "    \"\"\"\n",
    "    Given a corpus and a model and a number of topics, \n",
    "    get the topic probability distribution for each document in the corpus \n",
    "    and use it to get the average topic proportions in that corpus for the model\n",
    "    \"\"\"\n",
    "    group_topic_proba = np.zeros(best_n_clusters)\n",
    "    topics = model[corpus]\n",
    "    for td in topics:\n",
    "        try:\n",
    "            group_topic_proba = group_topic_proba + np.array([t[1] for t in td])\n",
    "        except IndexError as e:\n",
    "            print(len(td), len(group_topic_proba))\n",
    "            print(group_topic_proba, td)\n",
    "            print(e)\n",
    "            print()\n",
    "    z = group_topic_proba / sum(group_topic_proba)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models, corpora\n",
    "mallet_path = 'Downloads/mallet-2.0.8/bin/mallet'\n",
    "def do_topics(content_full, best_n_clusters):\n",
    "    \"\"\"\n",
    "    Given a processed corpus and a number of topics, compute an LDA model with that number of topics\n",
    "    based on the given corpus.\n",
    "    \n",
    "    content_full should be a list of lists of words, where each list of words corresponds to a processed article.\n",
    "    best_n_clusters should be an integer - specifically, the number of topics you're looking for.\n",
    "    \n",
    "    this will return an LDA model (gensim object right now) and a corpus object (dict --> BOW for each article)\n",
    "    \"\"\"\n",
    "    dictionary = corpora.Dictionary(content_full)\n",
    "    # We filter out rare or overly common words here; \n",
    "    # note that since the number of documents per week is changing a lot, \n",
    "    # I make the filtering dependent on number of documents in the corpus.\n",
    "    dictionary.filter_extremes(no_below=int(len(content_full)/50), no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(text) for text in content_full]\n",
    "    lda_model = models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=best_n_clusters, optimize_interval=10, id2word=dictionary)\n",
    "    topic_keywords = []\n",
    "    for idx, topic in lda_model.show_topics(num_topics=10, num_words=20, formatted=False):\n",
    "        print('Topic: {} \\nWords: {}'.format(idx, [w[0] for w in topic]))\n",
    "        for my_word in [w[0] for w in topic]:\n",
    "            print(my_word)\n",
    "        topic_keywords.append([w[0] for w in topic])\n",
    "    p = get_topic_proportions(corpus, lda_model, best_n_clusters)\n",
    "    final = pd.DataFrame()\n",
    "    for w_idx in range(len(topic_keywords[0])):\n",
    "        final['word_{}'.format(str(w_idx))] = [topic_keywords[i][w_idx] for i in range(len(topic_keywords))]\n",
    "    final['proportions'] = p\n",
    "    return lda_model, p, final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'conservative')\n",
      "4\n",
      "{'20200109', '20200111'}\n",
      "Topic: 0 \n",
      "Words: ['determination', 'health', 'patient', 'friday', 'handling', 'considered', 'culprit', 'disease', 'sunday', 'chinese', 'thursday', 'researching', 'throat', 'thats', 'test', 'time', 'swab', 'city', 'dozen', 'viral']\n",
      "determination\n",
      "health\n",
      "patient\n",
      "friday\n",
      "handling\n",
      "considered\n",
      "culprit\n",
      "disease\n",
      "sunday\n",
      "chinese\n",
      "thursday\n",
      "researching\n",
      "throat\n",
      "thats\n",
      "test\n",
      "time\n",
      "swab\n",
      "city\n",
      "dozen\n",
      "viral\n",
      "Topic: 1 \n",
      "Words: ['state', 'preliminary', 'jianguo', 'released', 'test', 'municipal', 'pathogen', 'fro', 'culprit', 'city', 'virus', 'viral', 'researching', 'dozen', 'swab', 'sickened', 'say', 'death', 'time', 'throat']\n",
      "state\n",
      "preliminary\n",
      "jianguo\n",
      "released\n",
      "test\n",
      "municipal\n",
      "pathogen\n",
      "fro\n",
      "culprit\n",
      "city\n",
      "virus\n",
      "viral\n",
      "researching\n",
      "dozen\n",
      "swab\n",
      "sickened\n",
      "say\n",
      "death\n",
      "time\n",
      "throat\n",
      "Topic: 2 \n",
      "Words: ['time', 'stable', 'mysterious', 'made', 'preliminary', 'jianguo', 'city', 'coronaviru', 'collecting', 'dozen', 'culprit', 'virus', 'thats', 'researching', 'throat', 'test', 'swab', 'sickened', 'viral', 'sample']\n",
      "time\n",
      "stable\n",
      "mysterious\n",
      "made\n",
      "preliminary\n",
      "jianguo\n",
      "city\n",
      "coronaviru\n",
      "collecting\n",
      "dozen\n",
      "culprit\n",
      "virus\n",
      "thats\n",
      "researching\n",
      "throat\n",
      "test\n",
      "swab\n",
      "sickened\n",
      "viral\n",
      "sample\n",
      "Topic: 3 \n",
      "Words: ['medium', 'people', 'dozen', 'wuhan', 'determined', 'mysterious', 'virus', 'viral', 'time', 'throat', 'test', 'swab', 'sample', 'say', 'city', 'researcher', 'researching', 'total', 'sickened', 'culprit']\n",
      "medium\n",
      "people\n",
      "dozen\n",
      "wuhan\n",
      "determined\n",
      "mysterious\n",
      "virus\n",
      "viral\n",
      "time\n",
      "throat\n",
      "test\n",
      "swab\n",
      "sample\n",
      "say\n",
      "city\n",
      "researcher\n",
      "researching\n",
      "total\n",
      "sickened\n",
      "culprit\n",
      "Topic: 4 \n",
      "Words: ['commission', 'bbc', 'citing', 'discharged', 'swab', 'lead', 'patient', 'virus', 'dozen', 'culprit', 'city', 'thats', 'time', 'researching', 'test', 'sickened', 'say', 'death', 'viral', 'throat']\n",
      "commission\n",
      "bbc\n",
      "citing\n",
      "discharged\n",
      "swab\n",
      "lead\n",
      "patient\n",
      "virus\n",
      "dozen\n",
      "culprit\n",
      "city\n",
      "thats\n",
      "time\n",
      "researching\n",
      "test\n",
      "sickened\n",
      "say\n",
      "death\n",
      "viral\n",
      "throat\n",
      "Topic: 5 \n",
      "Words: ['sunday', 'wednesday', 'explains', 'thats', 'researching', 'dozen', 'culprit', 'city', 'virus', 'viral', 'time', 'throat', 'test', 'swab', 'sickened', 'say', 'sample', 'commission', 'people', 'suffering']\n",
      "sunday\n",
      "wednesday\n",
      "explains\n",
      "thats\n",
      "researching\n",
      "dozen\n",
      "culprit\n",
      "city\n",
      "virus\n",
      "viral\n",
      "time\n",
      "throat\n",
      "test\n",
      "swab\n",
      "sickened\n",
      "say\n",
      "sample\n",
      "commission\n",
      "people\n",
      "suffering\n",
      "Topic: 6 \n",
      "Words: ['investigation', 'medium', 'people', 'respiratory', 'authority', 'state', 'figure', 'virus', 'lead', 'sickened', 'stable', 'thursday', 'suffering', 'time', 'throat', 'city', 'saturday', 'viral', 'dozen', 'total']\n",
      "investigation\n",
      "medium\n",
      "people\n",
      "respiratory\n",
      "authority\n",
      "state\n",
      "figure\n",
      "virus\n",
      "lead\n",
      "sickened\n",
      "stable\n",
      "thursday\n",
      "suffering\n",
      "time\n",
      "throat\n",
      "city\n",
      "saturday\n",
      "viral\n",
      "dozen\n",
      "total\n",
      "Topic: 7 \n",
      "Words: ['preliminarily', 'saturday', 'researcher', 'identified', 'total', 'test', 'sample', 'rest', 'wuhan', 'time', 'suffering', 'sickened', 'city', 'virus', 'viral', 'dozen', 'say', 'swab', 'throat', 'thats']\n",
      "preliminarily\n",
      "saturday\n",
      "researcher\n",
      "identified\n",
      "total\n",
      "test\n",
      "sample\n",
      "rest\n",
      "wuhan\n",
      "time\n",
      "suffering\n",
      "sickened\n",
      "city\n",
      "virus\n",
      "viral\n",
      "dozen\n",
      "say\n",
      "swab\n",
      "throat\n",
      "thats\n",
      "Topic: 8 \n",
      "Words: ['suffering', 'death', 'report', 'blood', 'earlier', 'throat', 'dozen', 'city', 'virus', 'viral', 'time', 'test', 'swab', 'sickened', 'say', 'sample', 'researcher', 'wuhan', 'culprit', 'mysterious']\n",
      "suffering\n",
      "death\n",
      "report\n",
      "blood\n",
      "earlier\n",
      "throat\n",
      "dozen\n",
      "city\n",
      "virus\n",
      "viral\n",
      "time\n",
      "test\n",
      "swab\n",
      "sickened\n",
      "say\n",
      "sample\n",
      "researcher\n",
      "wuhan\n",
      "culprit\n",
      "mysterious\n",
      "Topic: 9 \n",
      "Words: ['viral', 'identified', 'researching', 'chinese', 'sickened', 'local', 'dozen', 'culprit', 'city', 'virus', 'wuhan', 'thats', 'throat', 'test', 'swab', 'say', 'sample', 'determined', 'time', 'people']\n",
      "viral\n",
      "identified\n",
      "researching\n",
      "chinese\n",
      "sickened\n",
      "local\n",
      "dozen\n",
      "culprit\n",
      "city\n",
      "virus\n",
      "wuhan\n",
      "thats\n",
      "throat\n",
      "test\n",
      "swab\n",
      "say\n",
      "sample\n",
      "determined\n",
      "time\n",
      "people\n",
      "\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "(2, 'mainstream')\n",
      "36\n",
      "{'20200107', '20200106', '20200110', '20200111', '20200109', '20200108'}\n",
      "Topic: 0 \n",
      "Words: ['death', 'report', 'scientist', 'investigating', 'health', 'central', 'condition', 'discovered', 'saturday', 'type', 'critical', 'commission', 'illness', 'mystery', 'infected', 'sickened', 'reported', 'country', 'strain', 'implicated']\n",
      "death\n",
      "report\n",
      "scientist\n",
      "investigating\n",
      "health\n",
      "central\n",
      "condition\n",
      "discovered\n",
      "saturday\n",
      "type\n",
      "critical\n",
      "commission\n",
      "illness\n",
      "mystery\n",
      "infected\n",
      "sickened\n",
      "reported\n",
      "country\n",
      "strain\n",
      "implicated\n",
      "Topic: 1 \n",
      "Words: ['patient', 'handful', 'dozen', 'whats', 'city', 'havent', 'mystery', 'hand', 'illness', 'developed', 'dec', 'appears', 'condition', 'discharged', 'hospitalized', 'severe', 'sars', 'worry', 'million', 'symptom']\n",
      "patient\n",
      "handful\n",
      "dozen\n",
      "whats\n",
      "city\n",
      "havent\n",
      "mystery\n",
      "hand\n",
      "illness\n",
      "developed\n",
      "dec\n",
      "appears\n",
      "condition\n",
      "discharged\n",
      "hospitalized\n",
      "severe\n",
      "sars\n",
      "worry\n",
      "million\n",
      "symptom\n",
      "Topic: 2 \n",
      "Words: ['pathogen', 'reported', 'state', 'identified', 'preliminary', 'investigation', 'type', 'medium', 'citing', 'thursday', 'discharged', 'respiratory', 'disease', 'scientist', 'illness', 'patient', 'condition', 'rest', 'sunday', 'local']\n",
      "pathogen\n",
      "reported\n",
      "state\n",
      "identified\n",
      "preliminary\n",
      "investigation\n",
      "type\n",
      "medium\n",
      "citing\n",
      "thursday\n",
      "discharged\n",
      "respiratory\n",
      "disease\n",
      "scientist\n",
      "illness\n",
      "patient\n",
      "condition\n",
      "rest\n",
      "sunday\n",
      "local\n",
      "Topic: 3 \n",
      "Words: ['news', 'shot', 'passenger', 'iranian', 'force', 'plane', 'offer', 'abc', 'photo', 'supreme', 'ago', 'iran', 'call', 'condolence', 'dead', 'international', 'investigation', 'leader', 'asylumseekers', 'eritrean']\n",
      "news\n",
      "shot\n",
      "passenger\n",
      "iranian\n",
      "force\n",
      "plane\n",
      "offer\n",
      "abc\n",
      "photo\n",
      "supreme\n",
      "ago\n",
      "iran\n",
      "call\n",
      "condolence\n",
      "dead\n",
      "international\n",
      "investigation\n",
      "leader\n",
      "asylumseekers\n",
      "eritrean\n",
      "Topic: 4 \n",
      "Words: ['city', 'viral', 'central', 'year', 'news', 'identified', 'unknown', 'sars', 'preliminary', 'beijing', 'raised', 'late', 'epidemic', 'infected', 'january', 'press', 'breaking', 'alert', 'killed', 'bug']\n",
      "city\n",
      "viral\n",
      "central\n",
      "year\n",
      "news\n",
      "identified\n",
      "unknown\n",
      "sars\n",
      "preliminary\n",
      "beijing\n",
      "raised\n",
      "late\n",
      "epidemic\n",
      "infected\n",
      "january\n",
      "press\n",
      "breaking\n",
      "alert\n",
      "killed\n",
      "bug\n",
      "Topic: 5 \n",
      "Words: ['worked', 'market', 'seafood', 'mysterious', 'investigate', 'image', 'huanan', 'erin', 'deciccagetty', 'closed', 'afflicted', 'read', 'min', 'family', 'gale', 'january', 'lauren', 'member', 'culprit', 'sanitize']\n",
      "worked\n",
      "market\n",
      "seafood\n",
      "mysterious\n",
      "investigate\n",
      "image\n",
      "huanan\n",
      "erin\n",
      "deciccagetty\n",
      "closed\n",
      "afflicted\n",
      "read\n",
      "min\n",
      "family\n",
      "gale\n",
      "january\n",
      "lauren\n",
      "member\n",
      "culprit\n",
      "sanitize\n",
      "Topic: 6 \n",
      "Words: ['rep', 'dmich', 'elissa_slotkin', 'iranianbacked', 'resolution', 'served', 'sponsor', 'interview', 'idea', 'government', 'foremost', 'power', 'expert', 'defense', 'daily', 'cia', 'capitol', 'mariana', 'give', 'secretary']\n",
      "rep\n",
      "dmich\n",
      "elissa_slotkin\n",
      "iranianbacked\n",
      "resolution\n",
      "served\n",
      "sponsor\n",
      "interview\n",
      "idea\n",
      "government\n",
      "foremost\n",
      "power\n",
      "expert\n",
      "defense\n",
      "daily\n",
      "cia\n",
      "capitol\n",
      "mariana\n",
      "give\n",
      "secretary\n",
      "Topic: 7 \n",
      "Words: ['mysterious', 'sars', 'related', 'january', 'scientist', 'culprit', 'cnn', 'gan', 'facebook', 'messenger', 'chat', 'world', 'whats_happening', 'unfolds', 'updated', 'find', 'dozen', 'cnna', 'edge', 'nectar']\n",
      "mysterious\n",
      "sars\n",
      "related\n",
      "january\n",
      "scientist\n",
      "culprit\n",
      "cnn\n",
      "gan\n",
      "facebook\n",
      "messenger\n",
      "chat\n",
      "world\n",
      "whats_happening\n",
      "unfolds\n",
      "updated\n",
      "find\n",
      "dozen\n",
      "cnna\n",
      "edge\n",
      "nectar\n",
      "Topic: 8 \n",
      "Words: ['disease', 'common', 'case', 'cold', 'patient', 'respiratory', 'sars', 'severe', 'infected', 'linked', 'marked', 'worked', 'lung', 'range', 'lesion', 'fiftynine', 'mystery', 'number', 'responsible', 'coronaviruses']\n",
      "disease\n",
      "common\n",
      "case\n",
      "cold\n",
      "patient\n",
      "respiratory\n",
      "sars\n",
      "severe\n",
      "infected\n",
      "linked\n",
      "marked\n",
      "worked\n",
      "lung\n",
      "range\n",
      "lesion\n",
      "fiftynine\n",
      "mystery\n",
      "number\n",
      "responsible\n",
      "coronaviruses\n",
      "Topic: 9 \n",
      "Words: ['illness', 'health', 'spread', 'human', 'pneumonialike', 'readily', 'causing', 'researcher', 'death', 'region', 'evidence', 'reading', 'agency', 'hong_kong', 'continue', 'identifies', 'asia', 'central', 'identified', 'story']\n",
      "illness\n",
      "health\n",
      "spread\n",
      "human\n",
      "pneumonialike\n",
      "readily\n",
      "causing\n",
      "researcher\n",
      "death\n",
      "region\n",
      "evidence\n",
      "reading\n",
      "agency\n",
      "hong_kong\n",
      "continue\n",
      "identifies\n",
      "asia\n",
      "central\n",
      "identified\n",
      "story\n",
      "\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "(3, 'conservative')\n",
      "16\n",
      "{'20200113', '20200117', '20200116', '20200115', '20200114', '20200118'}\n",
      "Topic: 0 \n",
      "Words: ['jan', 'returned', 'japan', 'man', 'pneumonia', 'recently', 'died', 'developed', 'cough', 'diagnosed', 'human', 'case', 'family', 'official', 'test', 'claim', 'fallen', 'severe', 'acute', 'positive']\n",
      "jan\n",
      "returned\n",
      "japan\n",
      "man\n",
      "pneumonia\n",
      "recently\n",
      "died\n",
      "developed\n",
      "cough\n",
      "diagnosed\n",
      "human\n",
      "case\n",
      "family\n",
      "official\n",
      "test\n",
      "claim\n",
      "fallen\n",
      "severe\n",
      "acute\n",
      "positive\n",
      "Topic: 1 \n",
      "Words: ['conducted', 'work', 'yearold', 'authority', 'cold', 'image', 'saturday', 'started', 'city', 'claim', 'organ', 'xray', 'official', 'human', 'diagnosed', 'may', 'possible', 'say', 'market', 'could']\n",
      "conducted\n",
      "work\n",
      "yearold\n",
      "authority\n",
      "cold\n",
      "image\n",
      "saturday\n",
      "started\n",
      "city\n",
      "claim\n",
      "organ\n",
      "xray\n",
      "official\n",
      "human\n",
      "diagnosed\n",
      "may\n",
      "possible\n",
      "say\n",
      "market\n",
      "could\n",
      "Topic: 2 \n",
      "Words: ['pneumonia', 'airport', 'berlin', 'heart', 'illness', 'yearold', 'province', 'drosten', 'avoid', 'place', 'people', 'condition', 'disease', 'severe', 'charite', 'diagnosed', 'determine', 'could', 'worked', 'xray']\n",
      "pneumonia\n",
      "airport\n",
      "berlin\n",
      "heart\n",
      "illness\n",
      "yearold\n",
      "province\n",
      "drosten\n",
      "avoid\n",
      "place\n",
      "people\n",
      "condition\n",
      "disease\n",
      "severe\n",
      "charite\n",
      "diagnosed\n",
      "determine\n",
      "could\n",
      "worked\n",
      "xray\n",
      "Topic: 3 \n",
      "Words: ['night', 'transmissible', 'medium', 'thai', 'official', 'japa', 'response', 'died', 'fortyone', 'wednesday', 'emergency', 'christian', 'adhanom', 'time', 'human', 'determine', 'investigation', 'cough', 'tuesday', 'xray']\n",
      "night\n",
      "transmissible\n",
      "medium\n",
      "thai\n",
      "official\n",
      "japa\n",
      "response\n",
      "died\n",
      "fortyone\n",
      "wednesday\n",
      "emergency\n",
      "christian\n",
      "adhanom\n",
      "time\n",
      "human\n",
      "determine\n",
      "investigation\n",
      "cough\n",
      "tuesday\n",
      "xray\n",
      "Topic: 4 \n",
      "Words: ['city', 'official', 'ill', 'confirms', 'spread', 'case', 'people', 'mystery', 'diagnosed', 'yearold', 'released', 'type', 'admitted', 'death', 'common', 'deve', 'treated', 'persistent', 'abnormal', 'commission']\n",
      "city\n",
      "official\n",
      "ill\n",
      "confirms\n",
      "spread\n",
      "case\n",
      "people\n",
      "mystery\n",
      "diagnosed\n",
      "yearold\n",
      "released\n",
      "type\n",
      "admitted\n",
      "death\n",
      "common\n",
      "deve\n",
      "treated\n",
      "persistent\n",
      "abnormal\n",
      "commission\n",
      "Topic: 5 \n",
      "Words: ['man', 'outbreak', 'patient', 'identified', 'chinese', 'mysterious', 'market', 'death', 'san_francisco', 'human', 'traced', 'fatality', 'case', 'medium', 'pneumonia', 'reported', 'sickened', 'state', 'seafood', 'victim']\n",
      "man\n",
      "outbreak\n",
      "patient\n",
      "identified\n",
      "chinese\n",
      "mysterious\n",
      "market\n",
      "death\n",
      "san_francisco\n",
      "human\n",
      "traced\n",
      "fatality\n",
      "case\n",
      "medium\n",
      "pneumonia\n",
      "reported\n",
      "sickened\n",
      "state\n",
      "seafood\n",
      "victim\n",
      "Topic: 6 \n",
      "Words: ['patient', 'identified', 'official', 'wholesale', 'authority', 'institute', 'xray', 'underlying', 'contracted', 'dec', 'ministry', 'labor', 'latest', 'brings', 'customer', 'arrive', 'market', 'case', 'chinese', 'municipal']\n",
      "patient\n",
      "identified\n",
      "official\n",
      "wholesale\n",
      "authority\n",
      "institute\n",
      "xray\n",
      "underlying\n",
      "contracted\n",
      "dec\n",
      "ministry\n",
      "labor\n",
      "latest\n",
      "brings\n",
      "customer\n",
      "arrive\n",
      "market\n",
      "case\n",
      "chinese\n",
      "municipal\n",
      "Topic: 7 \n",
      "Words: ['test', 'preliminary', 'xiong', 'amid', 'welfare', 'received', 'illness', 'reported', 'pneumonia', 'person', 'man', 'german', 'mystery', 'thailand', 'underlying', 'custom', 'center', 'diagnosis', 'director', 'organ']\n",
      "test\n",
      "preliminary\n",
      "xiong\n",
      "amid\n",
      "welfare\n",
      "received\n",
      "illness\n",
      "reported\n",
      "pneumonia\n",
      "person\n",
      "man\n",
      "german\n",
      "mystery\n",
      "thailand\n",
      "underlying\n",
      "custom\n",
      "center\n",
      "diagnosis\n",
      "director\n",
      "organ\n",
      "Topic: 8 \n",
      "Words: ['airport', 'screening', 'cdc', 'international', 'outbreak', 'major', 'jan', 'disease', 'arriving', 'air', 'enhanced', 'begin', 'los_angeles', 'kennedy', 'mysterious', 'new_york', 'region', 'significant', 'people', 'traveler']\n",
      "airport\n",
      "screening\n",
      "cdc\n",
      "international\n",
      "outbreak\n",
      "major\n",
      "jan\n",
      "disease\n",
      "arriving\n",
      "air\n",
      "enhanced\n",
      "begin\n",
      "los_angeles\n",
      "kennedy\n",
      "mysterious\n",
      "new_york\n",
      "region\n",
      "significant\n",
      "people\n",
      "traveler\n",
      "Topic: 9 \n",
      "Words: ['illness', 'keen', 'tedros', 'renal', 'regular', 'generally', 'report', 'racing', 'person', 'decides', 'number', 'ind', 'falling', 'showing', 'tuesday', 'xray', 'welfare', 'returned', 'sign', 'persistent']\n",
      "illness\n",
      "keen\n",
      "tedros\n",
      "renal\n",
      "regular\n",
      "generally\n",
      "report\n",
      "racing\n",
      "person\n",
      "decides\n",
      "number\n",
      "ind\n",
      "falling\n",
      "showing\n",
      "tuesday\n",
      "xray\n",
      "welfare\n",
      "returned\n",
      "sign\n",
      "persistent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "(3, 'liberal')\n",
      "3\n",
      "{'20200117'}\n"
     ]
    }
   ],
   "source": [
    "models_all = {}\n",
    "proportions_all = {}\n",
    "best_n_clusters = 10\n",
    "writer = pd.ExcelWriter('us_topics_aligned_0.xlsx') # or your preferred filename\n",
    "pol_leaning = set()\n",
    "full_dfs = []\n",
    "for index, gr in df_final.groupby(['dates_full', 'political_leaning']):\n",
    "    if len(gr) < 2:\n",
    "        continue\n",
    "    print(index)\n",
    "    print(len(gr))\n",
    "    dts = set(list(gr['dt_str']))\n",
    "    print(dts)\n",
    "    pol_leaning.add(index[1])\n",
    "    my_model, my_p, my_df = do_topics(gr['content_full'], best_n_clusters) \n",
    "    full_dfs.append(my_df.T)\n",
    "    ls_thing = [\"n = \" + str(len(gr['content_full']))]\n",
    "    full_dfs.append(pd.Series(ls_thing))\n",
    "    if len(pol_leaning) == 3:\n",
    "        df_full = pd.concat(full_dfs)\n",
    "        df_full.to_excel(writer, sheet_name=min(list(dts)) + ' - ' + max(list(dts)))\n",
    "        pol_leaning = set()\n",
    "        full_dfs = []\n",
    "    print()\n",
    "    print('---------------------------------------------')\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "writer.save() # make sure you run this line; otherwise the data doesn't get written to the sheet!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
